Fixes applied to make backendâ†’CLIâ†’Ollama integration reliable
=============================================================

Summary
-------
- Problem: The Python CLI (`main.py`) was interactive by default and did not exit after printing an assistant response when spawned by the Node backend. The backend waited for process exit and often timed out, producing 500 errors.
- Fix implemented: Added a single-shot non-interactive mode (`--once`) to the CLI and a lightweight `ask_once` path in the agent; updated `backend/server.js` to spawn `main.py --once` with a non-interactive environment and parse sentinel-marked assistant replies.

Files changed
-------------
- `app/src/cli/flags.py` â€” added `--once` flag
- `app/src/cli/cli.py` â€” handle `--once` (skip interactive prompts and print sentinel-wrapped response)
- `app/src/core/base.py` â€” added `ask_once(...)` (single-turn accumulation of AI output)
- `backend/server.js` â€” spawn Python with `--once`, set `env: { TERM: 'dumb', PYTHONUNBUFFERED: '1', NO_COLOR: '1' }`, and extract the assistant reply between sentinel markers

Why this works
--------------
- Running `main.py` with `--once` causes it to perform a single turn and exit. The backend receives a clean process exit and extracts only the model response.
- Setting `TERM=dumb` and `PYTHONUNBUFFERED=1` prevents interactive prompts and buffering issues caused by libraries that detect a TTY.

How to test locally (quick)
---------------------------
1) Start services (if not already running):

```bash
cd /home/zach/Jazz
./start_all.sh
```

2) Run the CLI in single-shot locally (sanity check):

```bash
PYTHONUNBUFFERED=1 TERM=dumb NO_COLOR=1 ./venv/bin/python3 main.py --once -p "What is 2+2?"
```

Expected: prints `JAZZ_SINGLE_SHOT_RESPONSE_START`, the assistant text, then `JAZZ_SINGLE_SHOT_RESPONSE_END` and exits.

3) Test the backend endpoint end-to-end:

```bash
# register or login to obtain a token
curl -X POST http://localhost:3000/api/auth/register -H "Content-Type: application/json" -d '{"username":"test","email":"t@example.com","password":"TestPass123!"}'
# extract token and call chat
curl -X POST http://localhost:3000/api/chat -H "Authorization: Bearer <TOKEN>" -H "Content-Type: application/json" -d '{"message":"What is 2+2?"}'
```

You should receive `assistantResponse` containing the model reply.

Next recommended improvements
---------------------------
- Quiet startup banners in `--once` mode to avoid extraneous stdout (optional; sentinel parsing already works).
- Add a JSON output flag for `--once` to make parsing 100% robust and machine-friendly.
- Add retry logic in the Python Ollama client for transient network errors.

If you want, I can implement the JSON `--once --json` output and suppress banners in `--once` mode â€” that will make the backend parsing simpler and robust.

Contact
-------
This file was generated by an automated assist during debugging; see commit(s) touching `app/src/cli` and `backend/server.js` for details.
# ðŸ”§ FIX: Jazz Backend Not Responding

Run these commands on the VM to fix the issue:

## Step 1: Update config.json with Ollama Host

```bash
nano ~/Jazz/config.json
```

Add this line after `"provider": "ollama"`:
```json
"ollama_host": "http://192.168.1.21:11434",
```

Full section should look like:
```json
{
    "provider": "ollama",
    "ollama_host": "http://192.168.1.21:11434",
    "provider_per_model": {
```

Save: `Ctrl+O`, `Enter`, `Ctrl+X`

---

## Step 2: Set up Python Virtual Environment

```bash
cd ~/Jazz

# Create venv if not exists
python3 -m venv venv

# Activate it
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Test Jazz CLI directly
python3 main.py "What is 2+2?"
```

If the last command works and you get a response, move to Step 3.

---

## Step 3: Restart All Services

```bash
cd ~/Jazz
./stop_all.sh
sleep 2
./start_all.sh
```

---

## Step 4: Test the Web UI

1. Open browser: http://VMIP:4200
2. Login
3. Send a message
4. Wait for response

---

## Step 5: Check Logs if Still Not Working

```bash
# Check backend logs
tail -50 ~/Jazz/backend/backend.log

# Check if Ollama is reachable
curl http://192.168.1.21:11434/api/tags

# Test Jazz CLI again
cd ~/Jazz && python3 main.py "test"
```

---

Send me the output of the logs if it still doesn't work!
