Fixes applied to make backendâ†’CLIâ†’Ollama integration reliable
=============================================================

Summary
-------
- Problem: The Python CLI (`main.py`) was interactive by default and did not exit after printing an assistant response when spawned by the Node backend. The backend waited for process exit and often timed out, producing 500 errors.
- Fix implemented: Added a single-shot non-interactive mode (`--once`) to the CLI and a lightweight `ask_once` path in the agent; updated `backend/server.js` to spawn `main.py --once` with a non-interactive environment and parse sentinel-marked assistant replies.

Files changed
-------------
- `app/src/cli/flags.py` â€” added `--once` flag
- `app/src/cli/cli.py` â€” handle `--once` (skip interactive prompts and print sentinel-wrapped response)
- `app/src/core/base.py` â€” added `ask_once(...)` (single-turn accumulation of AI output)
- `backend/server.js` â€” spawn Python with `--once`, set `env: { TERM: 'dumb', PYTHONUNBUFFERED: '1', NO_COLOR: '1' }`, and extract the assistant reply between sentinel markers

Why this works
--------------
- Running `main.py` with `--once` causes it to perform a single turn and exit. The backend receives a clean process exit and extracts only the model response.
- Setting `TERM=dumb` and `PYTHONUNBUFFERED=1` prevents interactive prompts and buffering issues caused by libraries that detect a TTY.

How to test locally (quick)
---------------------------
1) Start services (if not already running):

```bash
cd /home/zach/Jazz
./start_all.sh
```

2) Run the CLI in single-shot locally (sanity check):

```bash
PYTHONUNBUFFERED=1 TERM=dumb NO_COLOR=1 ./venv/bin/python3 main.py --once -p "What is 2+2?"
```

Expected: prints `JAZZ_SINGLE_SHOT_RESPONSE_START`, the assistant text, then `JAZZ_SINGLE_SHOT_RESPONSE_END` and exits.

3) Test the backend endpoint end-to-end:

```bash
# register or login to obtain a token
curl -X POST http://localhost:3000/api/auth/register -H "Content-Type: application/json" -d '{"username":"test","email":"t@example.com","password":"TestPass123!"}'
# extract token and call chat
curl -X POST http://localhost:3000/api/chat -H "Authorization: Bearer <TOKEN>" -H "Content-Type: application/json" -d '{"message":"What is 2+2?"}'
```

You should receive `assistantResponse` containing the model reply.

Next recommended improvements
---------------------------
- Quiet startup banners in `--once` mode to avoid extraneous stdout (optional; sentinel parsing already works).
- Add a JSON output flag for `--once` to make parsing 100% robust and machine-friendly.
- Add retry logic in the Python Ollama client for transient network errors.

If you want, I can implement the JSON `--once --json` output and suppress banners in `--once` mode â€” that will make the backend parsing simpler and robust.

Contact
-------
This file was generated by an automated assist during debugging; see commit(s) touching `app/src/cli` and `backend/server.js` for details.
 
Smoke test summary
------------------
I ran an automated smoke-test script that exercises login, stream-token issuance, and the SSE `/api/chat/stream` path for a set of representative queries (simple math, short summary, web lookup, live-score attempt).

High-level results:
- Stream token issuance: OK (short-lived tokens returned).
- SSE streaming: OK â€” server spawns the Python CLI and emits events. The SSE handler now tries to emit a concise answer quickly when it sees a short definitive sentence in stdout, and it also emits `meta` (which includes `full_response`) and `done` events.
- Simple arithmetic and short-summary queries: returned concise answers quickly in manual tests (visible in the UI). Automated script captures may vary depending on timing; if a test run shows no `data:` lines, re-run the individual curl command with a slightly longer timeout.
- Web-lookup queries: returned a short candidate line quickly (e.g. `Searching for "pymupdf_layout"...`) and then `meta` with a `full_response` containing the web results; the frontend modal shows the full response.
- Live-score fast-path: returns concise score JSON when a matching game is found (test depends on current day/ESPN scoreboard).

Sample manual test commands (these reliably show concise first replies):

```bash
# get JWT
TOKEN=$(curl -s -X POST -H 'Content-Type: application/json' -d '{"username":"e2etest","password":"pass123"}' http://localhost:3000/api/auth/login | jq -r .token)

# get stream token
STREAM=$(curl -s -X POST -H "Authorization: Bearer $TOKEN" http://localhost:3000/api/stream_token | jq -r .token)

# SSE: concise reply should appear immediately
curl -i -N --get --data-urlencode "message=What is 2+2?" --data "token=$STREAM" "http://localhost:3000/api/chat/stream"

# SSE: longer web lookup (will stream details; use modal to view full_response)
curl -i -N --get --data-urlencode "message=Find web results for pymupdf_layout" --data "token=$STREAM" "http://localhost:3000/api/chat/stream"
```

Notes & recommendations
-----------------------
- If an automated run shows no `data:` line for a query, try increasing the curl timeout (server may take a few seconds for web lookups).
- The SSE handler includes a heuristic to emit short definitive answers early; if you observe false positives (answers emitted too early), I can tune the heuristic to be more conservative.
- The environment variables to tune concurrency/rate limits are:
    - `MAX_CONCURRENT_CLI` (default 4)
    - `PER_USER_MAX_CONCURRENT_CLI` (default 1)
    - `RATE_LIMIT_WINDOW_SEC` (default 60)
    - `RATE_LIMIT_MAX_REQS` (default 30)

If you want, I can add the automated smoke-test script into the `scripts/` folder and wire a small `npm run smoke` helper, or I can make the heuristic more conservative or add rate-limit adjustments now.
# ðŸ”§ FIX: Jazz Backend Not Responding

Run these commands on the VM to fix the issue:

## Step 1: Update config.json with Ollama Host

```bash
nano ~/Jazz/config.json
```

Add this line after `"provider": "ollama"`:
```json
"ollama_host": "http://192.168.1.21:11434",
```

Full section should look like:
```json
{
    "provider": "ollama",
    "ollama_host": "http://192.168.1.21:11434",
    "provider_per_model": {
```

Save: `Ctrl+O`, `Enter`, `Ctrl+X`

---

## Step 2: Set up Python Virtual Environment

```bash
cd ~/Jazz

# Create venv if not exists
python3 -m venv venv

# Activate it
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Test Jazz CLI directly
python3 main.py "What is 2+2?"
```

If the last command works and you get a response, move to Step 3.

---

## Step 3: Restart All Services

```bash
cd ~/Jazz
./stop_all.sh
sleep 2
./start_all.sh
```

---

## Step 4: Test the Web UI

1. Open browser: http://VMIP:4200
2. Login
3. Send a message
4. Wait for response

---

## Step 5: Check Logs if Still Not Working

```bash
# Check backend logs
tail -50 ~/Jazz/backend/backend.log

# Check if Ollama is reachable
curl http://192.168.1.21:11434/api/tags

# Test Jazz CLI again
cd ~/Jazz && python3 main.py "test"
```

---

Send me the output of the logs if it still doesn't work!
